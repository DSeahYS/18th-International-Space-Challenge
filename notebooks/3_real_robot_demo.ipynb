{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Literal, Tuple\n",
    "from torch import Tensor\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapy as media\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "from einops import rearrange\n",
    "from jaxtyping import Float, Int\n",
    "\n",
    "from neural_jacobian_field.inference import (\n",
    "    CameraConfig,\n",
    "    ModelConfig,\n",
    "    ProgramConfig,\n",
    "    CameraContext,\n",
    "    load_model_cfg,\n",
    "    load_model,\n",
    "    load_nerfstudio_data,\n",
    "    parse_nerfstudio_camera,\n",
    "    plotting,\n",
    ")\n",
    "from neural_jacobian_field.rendering.geometry import (\n",
    "    get_world_rays_with_z,\n",
    "    project_world_coords_to_camera,\n",
    ")\n",
    "from neural_jacobian_field.data.dataset.dataset_toy_arm import DatasetToyArmPointTrack\n",
    "\n",
    "from neural_jacobian_field.visualization.view_interpolation import (\n",
    "    interpolate_intrinsics,\n",
    "    interpolate_pose,\n",
    "    reproj_best_torch,\n",
    ")\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_folder = Path(\n",
    "    \"/home/scene-rep-robot/scratch/neural-jacobian-field-private/\"\n",
    "    \"scripts/real_world/toy_arm/data/nerfstudio/05_19_2024\"\n",
    ")\n",
    "\n",
    "# TODO: modify cfg path and ckpt path appropriately.\n",
    "args = ProgramConfig(\n",
    "    ModelConfig(\n",
    "        # note this is relative to the\n",
    "        model_cfg_path=Path(\"../project/configurations/config\"),\n",
    "        model_ckpt_path=Path(\"../model_checkpoints/toy_arm/model.ckpt\"),\n",
    "    ),\n",
    "    CameraConfig(\n",
    "        ctxt_camera_idx=3,\n",
    "        trgt_camera_idx=7,\n",
    "        downscale_factor=1,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load camera information\"\"\"\n",
    "\n",
    "cameras, metadata = load_nerfstudio_data(\n",
    "    data_path=capture_folder,\n",
    "    downscale_factor=args.camera_config.downscale_factor,\n",
    ")\n",
    "\n",
    "camera_context = parse_nerfstudio_camera(\n",
    "    cameras=cameras,\n",
    "    ctxt_camera_idx=args.camera_config.ctxt_camera_idx,\n",
    "    trgt_camera_idx=args.camera_config.trgt_camera_idx,\n",
    "    device=args.device,\n",
    ")\n",
    "render_height = metadata[\"render_height\"]\n",
    "render_width = metadata[\"render_width\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load model\"\"\"\n",
    "\n",
    "ROBOT_ACTION_DIMENSIONS = {\n",
    "    \"toy_arm\": 6,\n",
    "}\n",
    "ROBOT_DATASET_CLASS = {\"toy_arm\": DatasetToyArmPointTrack}\n",
    "ROBOT_TYPES = Literal[\"toy_arm\", \"allegro\"]  # TODO: use enum\n",
    "\n",
    "curr_robot_type: ROBOT_TYPES = \"toy_arm\"\n",
    "curr_model_type = \"jacobian\"\n",
    "\n",
    "action_dim = ROBOT_ACTION_DIMENSIONS[curr_robot_type]\n",
    "robot_dataset_cls = ROBOT_DATASET_CLASS[curr_robot_type]\n",
    "\n",
    "model_cfg = load_model_cfg(\n",
    "    args.model_config.model_cfg_path,\n",
    "    overrides=[\n",
    "        f\"model={curr_robot_type}\",\n",
    "        f\"dataset={curr_robot_type}\",\n",
    "        f\"model.action_model_type={curr_model_type}\",\n",
    "        f\"model.action_dim={action_dim}\",\n",
    "        \"model.train_encoder=False\",\n",
    "        \"model.rendering.num_proposal_samples=[256]\",\n",
    "        \"model.rendering.num_nerf_samples=256\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "model = load_model(\n",
    "    model_cfg=model_cfg,\n",
    "    model_ckpt=args.model_config.model_ckpt_path,\n",
    "    device=args.device,\n",
    ")\n",
    "\n",
    "\n",
    "z_near = torch.tensor([robot_dataset_cls.near], device=args.device)\n",
    "z_far = torch.tensor([3.5], device=args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This part is tailor-made for visualizing the the chain-like structure of the toy robot\"\"\"\n",
    "\n",
    "from einops import reduce\n",
    "\n",
    "image_width, image_height = 640, 480\n",
    "\n",
    "\n",
    "def normalize_image(image):\n",
    "    return (image - image.min()) / (image.max() - image.min())\n",
    "\n",
    "\n",
    "def denormalize_torch_image(img_tensor: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Denormalize a [0, 1] torch image of shape [1, 3, H, W] to a numpy array of shape [H, W, 3]\n",
    "    with values in the range [0, 255].\n",
    "\n",
    "    Args:\n",
    "        img_tensor (torch.Tensor): Input tensor image of shape [1, 3, H, W].\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Denormalized image of shape [H, W, 3] with values in the range [0, 255].\n",
    "    \"\"\"\n",
    "    # Denormalize\n",
    "    img_numpy = img_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255.0\n",
    "    # Convert to uint8\n",
    "    img_numpy = img_numpy.astype(np.uint8)\n",
    "    return img_numpy\n",
    "\n",
    "\n",
    "def visualize_jacobian_chain_structure(\n",
    "    input_image_np: Int[np.ndarray, \"H W rgb\"],\n",
    "    input_joint_sensitivity_np: Float[Tensor, \"action_dim H W\"],\n",
    "    joint_colors_np: Float[np.ndarray, \"rgb action_dim\"],\n",
    "    debug: bool = False,\n",
    ") -> Tuple[Int[np.ndarray, \"H W rgb\"], Int[np.ndarray, \"H W rgb\"]]:\n",
    "\n",
    "    canvas_overlay = input_image_np.copy()\n",
    "\n",
    "    list_of_diff_masks = []\n",
    "    list_of_norm_projected = []\n",
    "\n",
    "    for i in range(1, 5):\n",
    "        prev_s = ((input_joint_sensitivity_np[i])).clip(0.08, 1.5)\n",
    "        next_s = ((input_joint_sensitivity_np[i + 1])).clip(0.08, 1.5)\n",
    "        diff = (prev_s - next_s).clip(0.01, 1)\n",
    "        diff = normalize_image(diff)\n",
    "        diff = cv2.resize(diff, (image_width, image_height))\n",
    "\n",
    "        if debug:\n",
    "            # create a 1x3 grid of plots\n",
    "            fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            # draw\n",
    "            ax[0].imshow(prev_s)\n",
    "            ax[0].set_title(\"prev_s\")\n",
    "            ax[1].imshow(next_s)\n",
    "            ax[1].set_title(\"next_s\")\n",
    "            ax[2].imshow(diff)\n",
    "            ax[2].set_title(\"diff\")\n",
    "            fig.colorbar(ax[0].imshow(prev_s), ax=ax[0])\n",
    "            fig.colorbar(ax[1].imshow(next_s), ax=ax[1])\n",
    "            fig.colorbar(ax[2].imshow(diff), ax=ax[2])\n",
    "            plt.show()\n",
    "\n",
    "        projected = diff[..., None] * np.array(joint_colors_np[i])\n",
    "        minima = reduce(projected, \"C H W -> C () ()\", \"min\")\n",
    "        maxima = reduce(projected, \"C H W -> C () ()\", \"max\")\n",
    "\n",
    "        norm_projected = (projected - minima) / (maxima - minima + 1e-10)\n",
    "        norm_projected = (norm_projected * 255).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "        if debug:\n",
    "            overlay = cv2.addWeighted(input_image_np, 0.5, norm_projected, 0.8, 0)\n",
    "            plt.imshow(overlay)\n",
    "            plt.show()\n",
    "\n",
    "        list_of_diff_masks.append(diff)\n",
    "        list_of_norm_projected.append(norm_projected)\n",
    "\n",
    "\n",
    "    next_s = cv2.resize(next_s, (image_width, image_height))\n",
    "\n",
    "    list_of_diff_masks.append(next_s.copy())\n",
    "    next_s = normalize_image(next_s.clip(0.25, 1.0))\n",
    "\n",
    "    projected = next_s[..., None] * np.array(joint_colors_np[i + 1])\n",
    "    minima = reduce(projected, \"C H W -> C () ()\", \"min\")\n",
    "    maxima = reduce(projected, \"C H W -> C () ()\", \"max\")\n",
    "\n",
    "    norm_projected = (projected - minima) / (maxima - minima + 1e-10)\n",
    "    norm_projected = (norm_projected * 255).clip(0, 255).astype(np.uint8)\n",
    "    list_of_norm_projected.append(norm_projected)\n",
    "\n",
    "    canvas_overlay = input_image_np.copy()\n",
    "    canvas_overlay = cv2.cvtColor(canvas_overlay, cv2.COLOR_RGB2RGBA)\n",
    "    canvas_overlay[..., 3] = 155\n",
    "    canvas_overlay = Image.fromarray(canvas_overlay)\n",
    "\n",
    "    canvas_white_bkgd = np.ones_like(input_image_np, dtype=np.uint8) * 255\n",
    "    canvas_white_bkgd = cv2.cvtColor(canvas_white_bkgd, cv2.COLOR_RGB2RGBA)\n",
    "\n",
    "    canvas_white_bkgd[..., 3] = 255\n",
    "    canvas_white_bkgd = Image.fromarray(canvas_white_bkgd)\n",
    "\n",
    "    for i in range(len(list_of_norm_projected)):\n",
    "        norm_projected_rgba = cv2.cvtColor(\n",
    "            list_of_norm_projected[i], cv2.COLOR_RGB2RGBA\n",
    "        )\n",
    "        diff = list_of_diff_masks[i]\n",
    "        diff = normalize_image(diff)\n",
    "\n",
    "        alpha_mask = diff * 1.5\n",
    "        alpha_mask = (alpha_mask.clip(0, 1) * 255).astype(np.uint8)\n",
    "        norm_projected_rgba[..., 3] = alpha_mask\n",
    "\n",
    "        overlay = Image.fromarray(norm_projected_rgba)\n",
    "\n",
    "        canvas_overlay.paste(overlay, (0, 0), overlay)\n",
    "        canvas_white_bkgd.paste(overlay, (0, 0), overlay)\n",
    "\n",
    "    return np.asarray(canvas_overlay), np.asarray(canvas_white_bkgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def render_single_view(\n",
    "    input_image_th: Float[Tensor, \"1 channel height width\"],\n",
    "    input_action_th: Float[Tensor, \"1 dim\"],\n",
    "    input_camera_context: CameraContext,\n",
    "    joint_colors_th: Float[Tensor, \"rgb action_dim\"],\n",
    "    trgt_ros: Optional[Float[Tensor, \"1 ray 3\"]] = None,\n",
    "    trgt_rds: Optional[Float[Tensor, \"1 ray 3\"]] = None,\n",
    "    patch_size: int = 2048,\n",
    "    compute_action_features: bool = True,\n",
    "    zero_out_sensitivity_below_thresh: float = 0.0,\n",
    "    zero_out_sensitivity_below_depth_percentile: int = 999,\n",
    "    device: torch.device = torch.device(\"cuda:0\"),\n",
    "    verbose: bool = True,\n",
    "):\n",
    "\n",
    "    trgt_extrinsics = input_camera_context[\"trgt_c2w\"].to(device).unsqueeze(0)\n",
    "    ctxt_extrinsics = input_camera_context[\"ctxt_c2w\"].to(device).unsqueeze(0)\n",
    "\n",
    "    trgt_intrinsics = input_camera_context[\"trgt_intr\"].to(device).unsqueeze(0)\n",
    "    ctxt_intrinsics = input_camera_context[\"ctxt_intr\"].to(device).unsqueeze(0)\n",
    "\n",
    "    coordinates = input_camera_context[\"coordinates\"].to(device)\n",
    "    coordinates = input_camera_context[\"coordinates\"].view(-1, 2).unsqueeze(0)\n",
    "\n",
    "    if trgt_ros is None and trgt_rds is None:\n",
    "        trgt_ros, trgt_rds, _ = get_world_rays_with_z(\n",
    "            coordinates, trgt_intrinsics, trgt_extrinsics\n",
    "        )\n",
    "\n",
    "\n",
    "    model_outputs = model.patch_render(\n",
    "        ctxt_image=input_image_th,\n",
    "        robot_action=input_action_th,\n",
    "        origins=trgt_ros,\n",
    "        directions=trgt_rds,\n",
    "        ctxt_c2w=ctxt_extrinsics,\n",
    "        ctxt_intr=ctxt_intrinsics,\n",
    "        trgt_c2w=trgt_extrinsics,\n",
    "        trgt_intr=trgt_intrinsics,\n",
    "        z_near=z_near,\n",
    "        z_far=z_far,\n",
    "        patch_size=patch_size,\n",
    "        render_height=render_height,\n",
    "        render_width=render_width,\n",
    "        compute_action_features=compute_action_features,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    joint_sensitivity_th = plotting.compute_joint_sensitivity(\n",
    "        model_outputs[\"pred_feat\"].squeeze(0),\n",
    "        camera_context[\"ctxt_c2w\"],\n",
    "    )\n",
    "\n",
    "    if zero_out_sensitivity_below_thresh > 0.0:\n",
    "        joint_sensitivity_th[\n",
    "            joint_sensitivity_th < zero_out_sensitivity_below_thresh\n",
    "        ] = 0\n",
    "\n",
    "    if zero_out_sensitivity_below_depth_percentile != 999:\n",
    "        depth_original = model_outputs[\"pred_depth\"].squeeze(0).cpu().numpy()\n",
    "        # create binary mask using the prescribed threshold\n",
    "        depth_thresh = np.percentile(\n",
    "            depth_original, zero_out_sensitivity_below_depth_percentile\n",
    "        )\n",
    "        depth_mask = (depth_original > depth_thresh).squeeze(-1)\n",
    "        joint_sensitivity_th[:, depth_mask] = 0\n",
    "\n",
    "    if curr_robot_type == \"toy_arm\":\n",
    "        input_image_np = denormalize_torch_image(input_image_th)\n",
    "        pred_jacobian_overlay_rgb, pred_jacobian_rgb = visualize_jacobian_chain_structure(\n",
    "            input_image_np=input_image_np,\n",
    "            input_joint_sensitivity_np=joint_sensitivity_th.cpu().numpy(),\n",
    "            joint_colors_np=joint_colors_th.T.cpu().numpy(),\n",
    "        )\n",
    "        model_outputs[\"pred_jacobian_overlay_rgb\"] = pred_jacobian_overlay_rgb\n",
    "\n",
    "    else:\n",
    "        pred_jacobian_rgb = plotting.visualize_joint_sensitivity(\n",
    "            joint_sensitivity_th=joint_sensitivity_th,\n",
    "            joint_colors_th=joint_colors_th,\n",
    "        )\n",
    "\n",
    "    model_outputs[\"joint_sensitivity\"] = joint_sensitivity_th\n",
    "    model_outputs[\"pred_jacobian_rgb\"] = pred_jacobian_rgb\n",
    "\n",
    "    return model_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_interpolated_view(\n",
    "    input_image: Float[Tensor, \"() channel height width\"],\n",
    "    input_action: Float[Tensor, \"() dim\"],\n",
    "    input_camera_context: CameraContext,\n",
    "    joint_colors_th: Float[Tensor, \"rgb action_dim\"],\n",
    "    src_steps: Tensor,\n",
    "    src_weights: Tensor,\n",
    "    list_of_drawing_loc_x: List[Tensor],\n",
    "    list_of_drawing_loc_y: List[Tensor],\n",
    "    list_of_ray_positions: List[Tensor],\n",
    "    list_of_ray_positions_warped: List[Tensor],\n",
    "    patch_size: int = 2048,\n",
    "    zero_out_sensitivity_below_thresh: float = 0.0,\n",
    "    zero_out_sensitivity_below_depth_percentile: int = 999,\n",
    "    num_frames_to_interpolate: int = 30,\n",
    "    # flow parameters\n",
    "    flow_vector_length_multiplier: int = 50,\n",
    "    flow_vector_line_thickness: int = 3,\n",
    "    # misc\n",
    "    device: torch.device = torch.device(\"cuda:0\"),\n",
    "    verbose: bool = True,\n",
    "):\n",
    "\n",
    "    trgt_extrinsics = input_camera_context[\"trgt_c2w\"].to(device)\n",
    "    ctxt_extrinsics = input_camera_context[\"ctxt_c2w\"].to(device)\n",
    "\n",
    "    trgt_intrinsics = input_camera_context[\"trgt_intr\"].to(device)\n",
    "    ctxt_intrinsics = input_camera_context[\"ctxt_intr\"].to(device)\n",
    "\n",
    "    trgt_intrinsics_raw = input_camera_context[\"trgt_intr_raw\"].to(device)\n",
    "    ctxt_intrinsics_raw = input_camera_context[\"ctxt_intr_raw\"].to(device)\n",
    "\n",
    "    coordinates = input_camera_context[\"coordinates\"].to(device)\n",
    "    coordinates = input_camera_context[\"coordinates\"].view(-1, 2)[None]\n",
    "\n",
    "    output_video_dict = {\n",
    "        k: []\n",
    "        for k in (\n",
    "            \"pred_rgb\",\n",
    "            \"pred_depth_rgb\",\n",
    "            \"pred_jacobian_rgb\",\n",
    "            \"pred_flow_rgb\",\n",
    "            \"pred_arrow_rgb\",\n",
    "        )\n",
    "    }\n",
    "    output_video_dict[\"pred_arrow_rgb\"] = [\n",
    "        [] for _ in range(len(list_of_drawing_loc_x))\n",
    "    ]\n",
    "\n",
    "    for t in tqdm.tqdm(\n",
    "        torch.linspace(0, 1, num_frames_to_interpolate), desc=\"Rendering video frames\"\n",
    "    ):\n",
    "        # Apply smoothing.\n",
    "        t = (math.cos(math.pi * (t.item() + 1)) + 1) / 2\n",
    "\n",
    "        # Video validation always uses a batch size of 1.\n",
    "        interp_trgt_c2w = interpolate_pose(\n",
    "            trgt_extrinsics,\n",
    "            ctxt_extrinsics,\n",
    "            t,\n",
    "        )[None]\n",
    "\n",
    "        interp_trgt_intr = interpolate_intrinsics(\n",
    "            trgt_intrinsics,\n",
    "            ctxt_intrinsics,\n",
    "            t,\n",
    "        )[None]\n",
    "\n",
    "        interp_trgt_intr_raw = interpolate_intrinsics(\n",
    "            trgt_intrinsics_raw,\n",
    "            ctxt_intrinsics_raw,\n",
    "            t,\n",
    "        )[None]\n",
    "\n",
    "        origins, directions, _ = get_world_rays_with_z(\n",
    "            coordinates, interp_trgt_intr, interp_trgt_c2w\n",
    "        )\n",
    "\n",
    "        xy_B_map_for_t = reproj_best_torch(\n",
    "            src_steps=src_steps,\n",
    "            src_weights=src_weights,\n",
    "            src_c2w=ctxt_extrinsics,\n",
    "            tgt_c2w=interp_trgt_c2w[0],\n",
    "            src_intrinsics=ctxt_intrinsics_raw,\n",
    "            tgt_intrinsics=interp_trgt_intr_raw[0],\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            total_pred_outputs = render_single_view(\n",
    "                input_image_th=input_image,\n",
    "                input_action_th=input_action,\n",
    "                input_camera_context=input_camera_context,\n",
    "                joint_colors_th=joint_colors_th,\n",
    "                trgt_ros=origins,\n",
    "                trgt_rds=directions,\n",
    "                compute_action_features=True,\n",
    "                zero_out_sensitivity_below_thresh=zero_out_sensitivity_below_thresh,\n",
    "                zero_out_sensitivity_below_depth_percentile=zero_out_sensitivity_below_depth_percentile,\n",
    "                patch_size=patch_size,\n",
    "                device=device,\n",
    "                verbose=verbose,\n",
    "            )\n",
    "\n",
    "            pred_rgb = total_pred_outputs[\"pred_rgb\"].cpu().numpy()[0]\n",
    "            pred_flow_rgb = total_pred_outputs[\"pred_flow_rgb\"].cpu().numpy()[0]\n",
    "            pred_depth_rgb = total_pred_outputs[\"pred_depth_rgb\"].cpu().numpy()[0]\n",
    "            pred_jacobian_rgb = total_pred_outputs[\"pred_jacobian_rgb\"]\n",
    "\n",
    "            output_video_dict[\"pred_rgb\"].append(pred_rgb)\n",
    "            output_video_dict[\"pred_flow_rgb\"].append(pred_flow_rgb)\n",
    "            output_video_dict[\"pred_depth_rgb\"].append(pred_depth_rgb)\n",
    "            output_video_dict[\"pred_jacobian_rgb\"].append(pred_jacobian_rgb)\n",
    "\n",
    "            for item_idx in range(len(list_of_drawing_loc_x)):\n",
    "                drawing_loc_x = list_of_drawing_loc_x[item_idx]\n",
    "                drawing_loc_y = list_of_drawing_loc_y[item_idx]\n",
    "                ray_positions = list_of_ray_positions[item_idx]\n",
    "                ray_positions_warped = list_of_ray_positions_warped[item_idx]\n",
    "\n",
    "                pix_to_draw_x, pix_to_draw_y = xy_B_map_for_t[\n",
    "                    drawing_loc_y, drawing_loc_x\n",
    "                ].split(1, dim=-1)\n",
    "\n",
    "                uv = project_world_coords_to_camera(\n",
    "                    ray_positions, interp_trgt_c2w, interp_trgt_intr_raw\n",
    "                )\n",
    "                uv_warped = project_world_coords_to_camera(\n",
    "                    ray_positions_warped, interp_trgt_c2w, interp_trgt_intr_raw\n",
    "                )\n",
    "\n",
    "                optical_flow_projected = uv_warped - uv\n",
    "\n",
    "                flow_arrow_rgb = plotting.draw_flow_on_image(\n",
    "                    curr_frame=pred_jacobian_rgb[..., :3].copy(),\n",
    "                    pix_to_draw_y=pix_to_draw_y,\n",
    "                    pix_to_draw_x=pix_to_draw_x,\n",
    "                    flow_pred_combined=optical_flow_projected,\n",
    "                    color_map_choice=\"cool\",\n",
    "                    length_multiplier=flow_vector_length_multiplier,\n",
    "                    line_thickness=flow_vector_line_thickness,\n",
    "                    use_norm=True,\n",
    "                )\n",
    "\n",
    "                output_video_dict[\"pred_arrow_rgb\"][item_idx].append(flow_arrow_rgb)\n",
    "\n",
    "    # if list_of_drawing_loc_x is not None:\n",
    "\n",
    "    output_video_dict[\"pred_arrow_rgb\"] = [\n",
    "        np.stack(x, axis=0) for x in output_video_dict[\"pred_arrow_rgb\"]\n",
    "    ]\n",
    "\n",
    "    for key in (\"pred_rgb\", \"pred_depth_rgb\", \"pred_flow_rgb\", \"pred_jacobian_rgb\"):\n",
    "        output_video_dict[key] = np.stack(output_video_dict[key], axis=0)  # type: ignore\n",
    "\n",
    "    return output_video_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define joint colors\n",
    "joint_colors_np = [\n",
    "    [0.5, 0.8, 0.2],\n",
    "    [0.9, 0.2, 0.0],\n",
    "    [0, 0.8, 0],\n",
    "    [1.0, 0.0, 1.0],\n",
    "    [0, 0, 1],\n",
    "    [0.1, 0.9, 0.7],\n",
    "]\n",
    "\n",
    "# Invert joint colors\n",
    "joint_colors_np = 1 - np.array(joint_colors_np)\n",
    "\n",
    "# Display the joint colors\n",
    "plt.imshow(np.array([joint_colors_np]))\n",
    "plt.title(\"Inverted Joint Colors\")\n",
    "plt.show()\n",
    "\n",
    "# Convert to torch tensor and move to the specified device\n",
    "joint_colors_th = torch.tensor(joint_colors_np).T.to(args.device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load some data and start inference\"\"\"\n",
    "from neural_jacobian_field.utils.io import numpy_to_torch_image\n",
    "from neural_jacobian_field.utils import convention\n",
    "\n",
    "raw_capture_folder = Path(str(capture_folder).replace(\"nerfstudio\", \"raw\"))\n",
    "\n",
    "demo_filename = raw_capture_folder / \"000325.npz\"\n",
    "demo_data = np.load(demo_filename)\n",
    "\n",
    "### (1) load image\n",
    "frame_idx = 5 # TODO: configurize this \n",
    "view_idx = args.camera_config.ctxt_camera_idx\n",
    "\n",
    "input_image_np = demo_data[\"color\"][frame_idx][view_idx]\n",
    "input_image_np[:] = input_image_np[..., ::-1]\n",
    "input_image_th = numpy_to_torch_image(input_image_np).to(args.device)\n",
    "input_action_th = torch.zeros((6,)).to(args.device)\n",
    "\n",
    "plt.imshow(input_image_np)\n",
    "plt.show()\n",
    "\n",
    "### (2) load action\n",
    "trgt_frame_idx = -1\n",
    "joint_pos_t0 = torch.from_numpy(demo_data[\"joint_pos\"][frame_idx])\n",
    "joint_pos_t1 = torch.from_numpy(demo_data[\"joint_pos\"][trgt_frame_idx])\n",
    "\n",
    "joint_positions = metadata[\"joint_positions\"]\n",
    "joint_positions_np = torch.stack(list(joint_positions.values()), dim=0)\n",
    "# normalize action to [0, 1]\n",
    "servo_pos_min = joint_positions_np.min(0).values.float()\n",
    "servo_pos_max = joint_positions_np.max(0).values.float()\n",
    "joint_pos_t0 = convention.normalize(joint_pos_t0, servo_pos_min, servo_pos_max)\n",
    "joint_pos_t1 = convention.normalize(joint_pos_t1, servo_pos_min, servo_pos_max)\n",
    "\n",
    "input_action_th = (joint_pos_t1 - joint_pos_t0).to(args.device) / 2.0\n",
    "input_action_th = input_action_th.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We are reloading cameras to make trgt = ctxt\n",
    "camera_context = parse_nerfstudio_camera(\n",
    "    cameras=cameras,\n",
    "    ctxt_camera_idx=args.camera_config.ctxt_camera_idx,\n",
    "    trgt_camera_idx=args.camera_config.ctxt_camera_idx,\n",
    "    device=args.device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We are reloading cameras to make trgt = ctxt\n",
    "camera_context = parse_nerfstudio_camera(\n",
    "    cameras=cameras,\n",
    "    ctxt_camera_idx=args.camera_config.ctxt_camera_idx,\n",
    "    trgt_camera_idx=args.camera_config.ctxt_camera_idx,\n",
    "    device=args.device,\n",
    ")\n",
    "\n",
    "render_height = metadata[\"render_height\"]\n",
    "render_width = metadata[\"render_width\"]\n",
    "\n",
    "list_of_drawing_loc_x = []\n",
    "list_of_drawing_loc_y = []\n",
    "list_of_ray_positions = []\n",
    "list_of_ray_positons_warped = []\n",
    "\n",
    "num_arrows_to_draw = 10\n",
    "for joint_activation_idx in range(1, 6):\n",
    "    flow_mask_thresh = 0.28 if joint_activation_idx in [1, 2] else 0.34\n",
    "\n",
    "    input_action_place_holder_th = torch.zeros_like(input_action_th).to(args.device)\n",
    "    input_action_place_holder_th[:, joint_activation_idx] = 1.0\n",
    "\n",
    "    camera_context[\"ctxt_intr_raw\"] = cameras.get_intrinsics_matrices()[\n",
    "        args.camera_config.ctxt_camera_idx\n",
    "    ].to(args.device)\n",
    "\n",
    "    input_image_th = numpy_to_torch_image(input_image_np).unsqueeze(0).to(args.device)\n",
    "\n",
    "    ctxt_extrinsics = camera_context[\"ctxt_c2w\"].to(args.device)\n",
    "    trgt_extrinsics = ctxt_extrinsics.clone()\n",
    "\n",
    "    ctxt_intrinsics = camera_context[\"ctxt_intr\"].to(args.device)\n",
    "    trgt_intrinsics = camera_context[\"trgt_intr\"].to(args.device)\n",
    "\n",
    "    coordinates = camera_context[\"coordinates\"].to(args.device)\n",
    "    coordinates = camera_context[\"coordinates\"].view(-1, 2)\n",
    "\n",
    "    trgt_ros, trgt_rds, _ = get_world_rays_with_z(\n",
    "        coordinates[None], trgt_intrinsics[None], trgt_extrinsics[None]\n",
    "    )\n",
    "\n",
    "    model_outputs = render_single_view(\n",
    "        input_image_th=input_image_th,\n",
    "        input_action_th=input_action_place_holder_th,\n",
    "        input_camera_context=camera_context,\n",
    "        joint_colors_th=joint_colors_th,\n",
    "        trgt_ros=trgt_ros,\n",
    "        trgt_rds=trgt_rds,\n",
    "        patch_size=2048,\n",
    "        compute_action_features=True,\n",
    "        zero_out_sensitivity_below_depth_percentile=90,\n",
    "    )\n",
    "\n",
    "    joint_sensitivity_th = model_outputs[\"joint_sensitivity\"]\n",
    "    pred_jacobian_overlay_rgb = model_outputs[\"pred_jacobian_overlay_rgb\"]\n",
    "\n",
    "    normalized_flow = torch.norm(model_outputs[\"pred_flow\"], dim=1, p=2, keepdim=True)\n",
    "    adaptive_thresh = (normalized_flow.max() * flow_mask_thresh).item()\n",
    "    flow_mask = (normalized_flow > adaptive_thresh).cpu().numpy().squeeze()\n",
    "    # get indices that are true\n",
    "    sensitivity_mask = joint_sensitivity_th.mean(0).cpu().numpy() > flow_mask_thresh\n",
    "    flow_mask *= sensitivity_mask\n",
    "\n",
    "    # apply filtering\n",
    "    drawing_loc_y, drawing_loc_x = flow_mask.nonzero()\n",
    "    indices = np.arange(len(drawing_loc_y))\n",
    "    rand_indices = np.random.choice(indices, num_arrows_to_draw, replace=False)\n",
    "\n",
    "    # rand_indices = np.random.choice(len(drawing_loc_y), num_to_draw, replace=False)\n",
    "    drawing_loc_y = drawing_loc_y[rand_indices]\n",
    "    drawing_loc_x = drawing_loc_x[rand_indices]\n",
    "    # break\n",
    "\n",
    "    ray_positions = (\n",
    "        model_outputs[\"pred_ray_positions\"]\n",
    "        .clone()[0, :, drawing_loc_y, drawing_loc_x]\n",
    "        .T\n",
    "    )\n",
    "    ray_positions_warped = (\n",
    "        model_outputs[\"pred_ray_positions_warped\"]\n",
    "        .clone()[0, :, drawing_loc_y, drawing_loc_x]\n",
    "        .T\n",
    "    )\n",
    "    trgt_intrinsics_raw = camera_context[\"trgt_intr_raw\"].to(args.device).unsqueeze(0)\n",
    "    ctxt_intrinsics_raw = camera_context[\"ctxt_intr_raw\"].to(args.device).unsqueeze(0)\n",
    "    ctxt_intrinsics = camera_context[\"ctxt_intr\"].to(args.device).unsqueeze(0)\n",
    "    ctxt_extrinsics = camera_context[\"ctxt_c2w\"].to(args.device).unsqueeze(0)\n",
    "\n",
    "    uv = project_world_coords_to_camera(\n",
    "        ray_positions, ctxt_extrinsics, ctxt_intrinsics_raw\n",
    "    )\n",
    "    uv_warped = project_world_coords_to_camera(\n",
    "        ray_positions_warped, ctxt_extrinsics, ctxt_intrinsics_raw\n",
    "    )\n",
    "    optical_flow = uv_warped - uv\n",
    "\n",
    "    flow_arrow_drawn_on_jacobian = model_outputs[\"pred_jacobian_rgb\"].copy()\n",
    "    for i, (x, y) in enumerate(zip(drawing_loc_x, drawing_loc_y)):\n",
    "        flow = optical_flow[:, i, :].squeeze()\n",
    "\n",
    "        flow = flow / torch.norm(flow) * 25\n",
    "\n",
    "        y1, x1 = y, x\n",
    "        y2, x2 = int(y + flow[1].item()), int(x + flow[0].item())\n",
    "\n",
    "        cv2.arrowedLine(\n",
    "            flow_arrow_drawn_on_jacobian,\n",
    "            (x1, y1),\n",
    "            (x2, y2),\n",
    "            (0, 255, 255),\n",
    "            2,\n",
    "            tipLength=0.2,\n",
    "        )\n",
    "\n",
    "    media.show_images(\n",
    "        [pred_jacobian_overlay_rgb, flow_mask, flow_arrow_drawn_on_jacobian],\n",
    "        height=render_height // 2,\n",
    "        width=render_width // 2,\n",
    "    )\n",
    "\n",
    "    list_of_drawing_loc_x.append(drawing_loc_x)\n",
    "    list_of_drawing_loc_y.append(drawing_loc_y)\n",
    "    list_of_ray_positions.append(ray_positions)\n",
    "    list_of_ray_positons_warped.append(ray_positions_warped)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
